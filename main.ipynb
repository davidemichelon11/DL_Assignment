{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/DL_Assignment/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "uY03c6gbuYAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4nCxw3XdPwwv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract data and create dataset"
      ],
      "metadata": {
        "id": "57Y8geD1uccJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "3C4tGtJuITp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80542d60-e6b4-419f-bb8a-6b5711bd6db5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o gdrive/MyDrive/Adaptiope.zip "
      ],
      "metadata": {
        "id": "Jn9EIurhutPN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8GPGsSMGRdHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa32879e-f817-4be3-eb71-ceb5af80645e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:03<00:00,  6.20it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.76it/s]\n"
          ]
        }
      ],
      "source": [
        "classes = [\"/backpack\", \"/bookcase\", \"/car jack\", \"/comb\", \"/crown\", \"/file cabinet\", \"/flat iron\", \"/game controller\", \"/glasses\",\n",
        "           \"/helicopter\", \"/ice skates\", \"/letter tray\", \"/monitor\", \"/mug\", \"/network switch\", \"/over-ear headphones\", \"/pen\",\n",
        "           \"/purse\", \"/stand mixer\", \"/stroller\"]\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  os.makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = ''.join((d, c))\n",
        "    c_target = ''.join((td, c))\n",
        "    shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataloader"
      ],
      "metadata": {
        "id": "FVrMqpSCyxwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2fkEbfTGVE-0"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, product_root, real_root):\n",
        "  # resizing and cropping\n",
        "  # prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                 # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # normalize with ImageNet mean\n",
        "  transform = T.Compose(transform)                            # compose the above transformations into one\n",
        "    \n",
        "  # load data\n",
        "  dataset_prod = torchvision.datasets.ImageFolder(root=product_root, transform=transform)\n",
        "  dataset_real = torchvision.datasets.ImageFolder(root=real_root, transform=transform)\n",
        "  \n",
        "  # create train and test splits (80/20)\n",
        "  num_samples = len(dataset_prod) # same number of samples in this dataset\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  train_data_prod, test_data_prod = torch.utils.data.random_split(dataset_prod, [training_samples, test_samples])\n",
        "  train_data_real, test_data_real = torch.utils.data.random_split(dataset_real, [training_samples, test_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader_prod = torch.utils.data.DataLoader(train_data_prod, batch_size, shuffle=True)\n",
        "  test_loader_prod = torch.utils.data.DataLoader(test_data_prod, batch_size, shuffle=False)\n",
        "  \n",
        "  train_loader_real = torch.utils.data.DataLoader(train_data_real, batch_size, shuffle=True)\n",
        "  test_loader_real = torch.utils.data.DataLoader(test_data_real, batch_size, shuffle=False)\n",
        "  \n",
        "  return (train_loader_prod, test_loader_prod), (train_loader_real, test_loader_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "kSH5iS7K1C2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_resnet(num_classes):\n",
        "\n",
        "  # load the pre-trained Alexnet\n",
        "  resnet = torchvision.models.resnet50(pretrained=True)\n",
        "  \n",
        "  # get the number of neurons in the second last layer\n",
        "  in_features = resnet.fc.in_features\n",
        "  \n",
        "  # re-initalize the output layer\n",
        "  resnet.fc = torch.nn.Linear(in_features=in_features, \n",
        "                                          out_features=num_classes)\n",
        "  \n",
        "  return resnet"
      ],
      "metadata": {
        "id": "XIlGuzqLNigi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify cost function and optimizer"
      ],
      "metadata": {
        "id": "OpK-Bq6d1FnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "zLj-c_tPIgCM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  # iterate through the layers of the network\n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('fc'):\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "  \n",
        "  # assign the distinct learning rates to each group of parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr=lr/10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "EMpc8dxrMmWc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "S_I6N6U81MDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, source_train_loader, target_train_loader, optimizer, \n",
        "                  cost_function, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  target_iter = iter(target_train_loader)\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.train()\n",
        "  for batch_idx, (inputs_source, targets) in enumerate(tqdm(source_train_loader)):\n",
        "    \n",
        "    # get target data. If the target iterator reaches the end, restart it\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_train_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    \n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    source_output, target_output = torch.split(outputs, \n",
        "                                               split_size_or_sections=outputs.shape[0] // 2, \n",
        "                                               dim=0)\n",
        "    \n",
        "    # apply the losses\n",
        "    ce_loss = cost_function(source_output, targets)\n",
        "    \n",
        "    loss = ce_loss # later we will add other losses\n",
        "    \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print statistics\n",
        "    source_samples += inputs_source.shape[0]\n",
        "    target_samples += inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    _, predicted = source_output.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_accuracy/source_samples*100\n",
        "\n",
        "\n",
        "def test_step(model, target_test_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(target_test_loader)):\n",
        "\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # print statistics\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "y5W55bDkMocB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute everything"
      ],
      "metadata": {
        "id": "J359b2Q73hhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "device='cuda:0'\n",
        "learning_rate=0.01\n",
        "weight_decay=0.000001\n",
        "momentum=0.9\n",
        "epochs=50\n",
        "\n",
        "prod_root = 'adaptiope_small/product_images'\n",
        "real_root = 'adaptiope_small/real_life'\n",
        "\n",
        "dataloaders_prod, dataloaders_real = get_data(batch_size, prod_root, real_root)\n",
        "train_loader_prod, test_loader_prod = dataloaders_real\n",
        "train_loader_real, test_loader_real = dataloaders_prod\n",
        "\n",
        "num_classes = len(set(train_loader_prod.dataset.dataset.targets))\n",
        "\n",
        "ResNet = initialize_resnet(num_classes).to(device)\n",
        "\n",
        "optimizer = get_optimizer(ResNet, learning_rate, weight_decay, momentum)\n",
        "\n",
        "cost_function = get_cost_function()\n",
        "\n",
        "for e in range(epochs):\n",
        "  print('Epoch: {}/{}'.format(e+1, epochs))\n",
        "  train_ce_loss, train_accuracy = training_step(model=ResNet,\n",
        "                                                source_train_loader=train_loader_prod,\n",
        "                                                target_train_loader=train_loader_real,\n",
        "                                                optimizer=optimizer, \n",
        "                                                cost_function=cost_function,\n",
        "                                                device=device)\n",
        "  \n",
        "  test_loss, test_accuracy = test_step(model=ResNet, \n",
        "                                       target_test_loader=test_loader_real, \n",
        "                                       cost_function=cost_function, \n",
        "                                       device=device)\n",
        "  \n",
        "  print('Train: CE loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_accuracy))\n",
        "  print('Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "YiR2HcGtM_Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zUJgeXww6Jas"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL_assignment_Giulio.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}