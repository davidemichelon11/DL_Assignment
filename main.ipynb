{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/DL_Assignment/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import makedirs\n",
        "from os import listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ffPEwcd91gLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_NH17GuP4Kp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Adaptiope.zip\" dataset/\n",
        "!ls dataset\n",
        "!unzip dataset/Adaptiope.zip\n",
        "!rm -rf adaptiope_small"
      ],
      "metadata": {
        "id": "vzobgOes1r5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir adaptiope_small"
      ],
      "metadata": {
        "id": "MXYoK2ks8l0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "classes = [\"/backpack\", \"/bookcase\", \"/car jack\", \"/comb\", \"/crown\", \"/file cabinet\", \"/flat iron\", \"/game controller\", \"/glasses\",\n",
        "           \"/helicopter\", \"/ice skates\", \"/letter tray\", \"/monitor\", \"/mug\", \"/network switch\", \"/over-ear headphones\", \"/pen\",\n",
        "           \"/purse\", \"/stand mixer\", \"/stroller\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = ''.join((d, c))\n",
        "    c_target = ''.join((td, c))\n",
        "    shutil.copytree(c_path, c_target)"
      ],
      "metadata": {
        "id": "dZm5jySz1wRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "YZ8-wSmICipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, img_root):\n",
        "  # resizing and cropping\n",
        "  # prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                 # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # normalize with ImageNet mean\n",
        "  transform = T.Compose(transform)                            # compose the above transformations into one\n",
        "    \n",
        "  # load data\n",
        "  dataset = torchvision.datasets.ImageFolder(root=img_root, transform=transform)\n",
        "  \n",
        "  # create train and test splits (80/20)\n",
        "  num_samples = len(dataset)\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, test_data = torch.utils.data.random_split(dataset, \n",
        "                                                           [training_samples, test_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "PD5EnkoSCZN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_resnet(num_classes):\n",
        "\n",
        "  # load the pre-trained Alexnet\n",
        "  resnet = torchvision.models.resnet50(pretrained=True)\n",
        "  \n",
        "  # get the number of neurons in the second last layer\n",
        "  in_features = resnet.fc.in_features\n",
        "  \n",
        "  # re-initalize the output layer\n",
        "  resnet.fc = torch.nn.Linear(in_features=in_features, \n",
        "                                          out_features=num_classes)\n",
        "  \n",
        "  return resnet"
      ],
      "metadata": {
        "id": "UZuSBpO6PUSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "28LBcVrtKiGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  # iterate through the layers of the network\n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('fc'):\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "  \n",
        "  # assign the distinct learning rates to each group of parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "454SYDrAKqTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_optimizer(initialize_resnet(20), 0.001, 0.1, 0.1)"
      ],
      "metadata": {
        "id": "4TwN-0VSRR8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to training mode: particularly important when using dropout!\n",
        "  net.train() \n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "XvOkDNmHLDz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "  num_classes: Number of classes in your dataset\n",
        "  visualization_name: Name of the visualization folder\n",
        "  img_root: The root folder of images\n",
        "'''\n",
        "\n",
        "def main(batch_size=30, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=65, \n",
        "         visualization_name='resnet_sgd', \n",
        "         img_root='adaptiope_small/product_images'):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # instantiates dataloaders\n",
        "  train_loader, test_loader = get_data(batch_size=batch_size, img_root=img_root)\n",
        "  \n",
        "  # instantiates the model\n",
        "  net = initialize_resnet(20).to(device)\n",
        "  \n",
        "  # instantiates the optimizer\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # instantiates the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # perform a preliminar step\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # add values to logger\n",
        "  writer.add_scalar('Loss/train_loss', train_loss, 0)\n",
        "  writer.add_scalar('Loss/test_loss', test_loss, 0)\n",
        "  writer.add_scalar('Accuracy/train_accuracy', train_accuracy, 0)\n",
        "  writer.add_scalar('Accuracy/test_accuracy', test_accuracy, 0)\n",
        "\n",
        "  # range over the number of epochs\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step(net, train_loader, optimizer, cost_function)\n",
        "  test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "ihVImByqLZds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "-yI1Ps0bWoAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UR7AFbb9X1_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}