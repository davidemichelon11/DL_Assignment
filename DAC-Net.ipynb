{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/DL_Assignment/blob/main/DAC-Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY03c6gbuYAT"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nCxw3XdPwwv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Y8geD1uccJ"
      },
      "source": [
        "# Extract data and create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C4tGtJuITp1",
        "outputId": "53c66e59-05f8-4261-cff3-5f7a224b93ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn9EIurhutPN"
      },
      "outputs": [],
      "source": [
        "!unzip -q -o gdrive/MyDrive/Adaptiope.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPGsSMGRdHX"
      },
      "outputs": [],
      "source": [
        "classes = [\"/backpack\", \"/bookcase\", \"/car jack\", \"/comb\", \"/crown\", \"/file cabinet\", \"/flat iron\", \"/game controller\", \"/glasses\",\n",
        "           \"/helicopter\", \"/ice skates\", \"/letter tray\", \"/monitor\", \"/mug\", \"/network switch\", \"/over-ear headphones\", \"/pen\",\n",
        "           \"/purse\", \"/stand mixer\", \"/stroller\"]\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  os.makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = ''.join((d, c))\n",
        "    c_target = ''.join((td, c))\n",
        "    shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrMqpSCyxwl"
      },
      "source": [
        "# Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fkEbfTGVE-0"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, product_root, real_root, num_ops=2, magnitude=10):\n",
        "  # resizing and cropping\n",
        "  # prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                 # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # normalize with ImageNet mean\n",
        "  \n",
        "  # compose the above transformations into weak and strong (which has RandAugment in addition)\n",
        "  transform_weak = T.Compose(transform.copy())\n",
        "  transform_strong = T.Compose(transform.copy())                           \n",
        "  transform_strong.transforms.insert(0, T.RandAugment(num_ops, magnitude))\n",
        "    \n",
        "  # load data\n",
        "  dataset_prod = torchvision.datasets.ImageFolder(root=product_root, transform=transform_weak)\n",
        "  dataset_prod_aug = torchvision.datasets.ImageFolder(root=product_root, transform=transform_strong)\n",
        "  dataset_real = torchvision.datasets.ImageFolder(root=real_root, transform=transform_weak)\n",
        "  dataset_real_aug = torchvision.datasets.ImageFolder(root=real_root, transform=transform_strong)\n",
        "  \n",
        "  # create train and test splits (80/20)\n",
        "  num_samples = len(dataset_prod) # same number of samples among datasets\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  train_data_prod, test_data_prod = torch.utils.data.random_split(dataset_prod, [training_samples, test_samples], generator=torch.Generator().manual_seed(1))\n",
        "  train_data_prod_aug, test_data_prod_aug = torch.utils.data.random_split(dataset_prod_aug, [training_samples, test_samples], generator=torch.Generator().manual_seed(1))\n",
        "  train_data_real, test_data_real = torch.utils.data.random_split(dataset_real, [training_samples, test_samples], generator=torch.Generator().manual_seed(2))\n",
        "  train_data_real_aug, test_data_real_aug = torch.utils.data.random_split(dataset_real_aug, [training_samples, test_samples], generator=torch.Generator().manual_seed(2))\n",
        "\n",
        "  # initialize sampler to get same data from weak and augment\n",
        "  sampler_prod = torch.utils.data.RandomSampler(train_data_prod, generator=torch.Generator().manual_seed(1))\n",
        "  sampler_prod_aug = torch.utils.data.RandomSampler(train_data_prod_aug, generator=torch.Generator().manual_seed(1))\n",
        "  sampler_real = torch.utils.data.RandomSampler(train_data_real, generator=torch.Generator().manual_seed(2))\n",
        "  sampler_real_aug = torch.utils.data.RandomSampler(train_data_real, generator=torch.Generator().manual_seed(2))\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader_prod = torch.utils.data.DataLoader(train_data_prod, batch_size, sampler=sampler_prod)\n",
        "  test_loader_prod = torch.utils.data.DataLoader(test_data_prod, batch_size, shuffle=False)\n",
        "  train_loader_prod_aug = torch.utils.data.DataLoader(train_data_prod_aug, batch_size, sampler=sampler_prod_aug)\n",
        "  test_loader_prod_aug = torch.utils.data.DataLoader(test_data_prod_aug, batch_size, shuffle=False)\n",
        "  \n",
        "  train_loader_real = torch.utils.data.DataLoader(train_data_real, batch_size, sampler=sampler_real)\n",
        "  test_loader_real = torch.utils.data.DataLoader(test_data_real, batch_size, shuffle=False)\n",
        "  train_loader_real_aug = torch.utils.data.DataLoader(train_data_real_aug, batch_size, sampler=sampler_real_aug)\n",
        "  test_loader_real_aug = torch.utils.data.DataLoader(test_data_real_aug, batch_size, shuffle=False)\n",
        "  \n",
        "  return (train_loader_prod, test_loader_prod), (train_loader_prod_aug, test_loader_prod_aug), (train_loader_real, test_loader_real), (train_loader_real_aug, test_loader_real_aug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSH5iS7K1C2I"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwWTeBXhiU7F"
      },
      "source": [
        "### Channel attention module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92eb78XZfVn_"
      },
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        \n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZSIzfjvyK0M"
      },
      "source": [
        "### Discriminator module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LiGaHeeyRue"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorConv(nn.Module):\n",
        "    def __init__(self, c1, num_convs=2):\n",
        "        super().__init__()\n",
        "        self.rev = GradientReversal()\n",
        "\n",
        "        dis_tower = []\n",
        "        for _ in range(num_convs):\n",
        "            dis_tower.append(\n",
        "                nn.Conv2d(\n",
        "                    c1,\n",
        "                    c1,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1\n",
        "                )\n",
        "            )\n",
        "            dis_tower.append(nn.GroupNorm(32, c1))\n",
        "            dis_tower.append(nn.ReLU())\n",
        "\n",
        "        self.add_module('dis_tower', nn.Sequential(*dis_tower))\n",
        "\n",
        "        self.cls_logits = nn.Conv2d(\n",
        "            c1, 1, kernel_size=3, stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        # initialization\n",
        "        for modules in [self.dis_tower, self.cls_logits]:\n",
        "            for l in modules.modules():\n",
        "                if isinstance(l, nn.Conv2d):\n",
        "                    torch.nn.init.normal_(l.weight, std=0.01)\n",
        "                    torch.nn.init.constant_(l.bias, 0)\n",
        "    \n",
        "    def forward(self, x, lambda_):\n",
        "        x = self.rev(x, lambda_)\n",
        "        \n",
        "        x = self.dis_tower(x)\n",
        "        x = self.cls_logits(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GradientReversal(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GradientReversal, self).__init__()\n",
        "\n",
        "    def forward(self, x, lambda_):\n",
        "        return GradientReversalFunction.apply(x, lambda_)\n",
        "\n",
        "\n",
        "class GradientReversalFunction(Function):\n",
        "    \"\"\"\n",
        "    Gradient Reversal Layer from:\n",
        "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
        "    Forward pass is the identity function. In the backward pass,\n",
        "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        lambda_ = ctx.lambda_\n",
        "        lambda_ = grads.new_tensor(lambda_)\n",
        "        dx = -lambda_ * grads\n",
        "        return dx, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke2vue9KicqL"
      },
      "source": [
        "### BasicBlock and Bottleneck modules of ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQvfrrXEhXkE"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_attention=True, output_attention=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.ca = ChannelAttention(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.output_attention = output_attention\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        if self.use_attention:\n",
        "            out_ca = self.ca(out)\n",
        "            out = residual + out_ca * out\n",
        "        else:\n",
        "            out = residual + out\n",
        "            out_ca = []\n",
        "        out = self.relu(out)\n",
        "        if self.output_attention:\n",
        "            return out, out_ca\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_attention=True, output_attention=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes,\n",
        "            planes,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, planes * self.expansion, kernel_size=1, bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.ca = ChannelAttention(planes * self.expansion)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.output_attention = output_attention\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        if self.use_attention:\n",
        "            out_ca = self.ca(out)\n",
        "            out = residual + out_ca * out\n",
        "        else:\n",
        "            out = residual + out\n",
        "            out_ca = []\n",
        "        out = self.relu(out)\n",
        "        if self.output_attention:\n",
        "            return out, out_ca\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ziG1Kpfk7tR"
      },
      "source": [
        "### ResNet architecture with channel attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIlGuzqLNigi"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers):\n",
        "        self.inplanes = 64\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], use_attention=False)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, use_attention=False)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, use_attention=True)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, use_attention=True)\n",
        "        self.disc = DiscriminatorConv(512 * block.expansion)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
        "\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv2d):\n",
        "        #         nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "        #         nn.init.constant_(m.weight, 1)\n",
        "        #         nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, use_attention=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.inplanes,\n",
        "                    planes * block.expansion,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, use_attention=use_attention))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == blocks - 1:\n",
        "                layers.append(block(self.inplanes, planes, output_attention=True, use_attention=use_attention))\n",
        "            else:\n",
        "                layers.append(block(self.inplanes, planes, use_attention=use_attention))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def featuremaps(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x, out_ca1 = self.layer1(x)\n",
        "        x, out_ca2 = self.layer2(x)\n",
        "        x, out_ca3 = self.layer3(x)\n",
        "        x, out_ca4 = self.layer4(x)\n",
        "        return x, [out_ca1, out_ca2, out_ca3, out_ca4]\n",
        "\n",
        "    def predictions(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, lambda_=0, gamma=0):\n",
        "        f, channel_attention = self.featuremaps(x)\n",
        "\n",
        "        # domain = self.disc(f, lambda_)\n",
        "        domain = self.disc(channel_attention[-1]*f*gamma + f*(1-gamma), lambda_)\n",
        "\n",
        "        v = self.avgpool(f)\n",
        "\n",
        "        out = self.predictions(v)\n",
        "        \n",
        "        return out, v, channel_attention, domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEw6vzLLlEO4"
      },
      "source": [
        "### Get model function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eWYZrEWkYTT"
      },
      "outputs": [],
      "source": [
        "def get_model(resnet=18, pretrained=False, num_classes=20):\n",
        "  if resnet == 18:\n",
        "    model = ResNet(block=BasicBlock, layers=[2, 2, 2, 2])\n",
        "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
        "  elif resnet == 34:\n",
        "    model = ResNet(block=BasicBlock, layers=[3, 4, 6, 3])\n",
        "    weights = ResNet34_Weights.IMAGENET1K_V1\n",
        "  elif resnet == 50:\n",
        "    model = ResNet(block=Bottleneck, layers=[3, 4, 6, 3])\n",
        "    weights = ResNet50_Weights.IMAGENET1K_V1\n",
        "  elif resnet == 101:\n",
        "    model = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n",
        "    weights = ResNet101_Weights.IMAGENET1K_V1\n",
        "  elif resnet == 152:\n",
        "    model = ResNet(block=Bottleneck, layers=[3, 8, 36, 3])\n",
        "    weights = ResNet152_Weights.IMAGENET1K_V1\n",
        "  else:\n",
        "    print(\"Unable to identify the ResNet architecture\")\n",
        "    return\n",
        "\n",
        "  if pretrained:\n",
        "    model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
        "\n",
        "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpK-Bq6d1FnV"
      },
      "source": [
        "# Specify optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMpc8dxrMmWc"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd=0, momentum=0, opt='Adam'):\n",
        "  if opt == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  elif opt == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npm-U0u-FtTR"
      },
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer, epochs, sched='cosine'):\n",
        "  if sched == 'cosine':\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "  \n",
        "  return scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_I6N6U81MDA"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJAtlKKEFl5j"
      },
      "outputs": [],
      "source": [
        "def training_step(model, source_train_loader, target_train_loader, optimizer, \n",
        "                  pseudo=0., attn=0., comp=0., adv=0., device='cuda:0', show_img=False, ep=False):\n",
        "  \n",
        "  source_train_loader, source_train_loader_aug = source_train_loader\n",
        "  target_train_loader, target_train_loader_aug = target_train_loader\n",
        "\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  weight_pseudo = pseudo\n",
        "  weight_attn = attn\n",
        "  weight_comp = comp\n",
        "  weight_adv = adv\n",
        "\n",
        "  ema_alpha = 0.999\n",
        "  tar_ca_last1 = torch.tensor(1.)\n",
        "  tar_ca_last2 = torch.tensor(1.)\n",
        "  src_ca_last1 = torch.tensor(1.)\n",
        "  src_ca_last2 = torch.tensor(1.)\n",
        "\n",
        "  conf_threshold = 0.95\n",
        "\n",
        "  mloss_source = torch.zeros(1)\n",
        "  mloss_pseudo = torch.zeros(1)\n",
        "  mloss_attn = torch.zeros(1)\n",
        "  mloss_comp = torch.zeros(1)\n",
        "  mloss_adv = torch.zeros(1)\n",
        "\n",
        "  if show_img:\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
        "  \n",
        "  target_iter = iter(target_train_loader)\n",
        "  target_iter_aug = iter(target_train_loader_aug)\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.train()\n",
        "\n",
        "  pbar = tqdm(zip(source_train_loader, source_train_loader_aug), total=len(source_train_loader))\n",
        "\n",
        "  epoch, epochs = ep\n",
        "  nb = len(source_train_loader)\n",
        "  max_iterations = nb * epochs\n",
        "\n",
        "  for i, ((inputs_source, targets_source), (inputs_source_aug, _)) in enumerate(pbar):\n",
        "    \n",
        "    # get target data. If the target iterator reaches the end, restart it\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target_aug, _ = next(target_iter_aug)\n",
        "    except:\n",
        "      target_iter = iter(target_train_loader)\n",
        "      target_iter_aug = iter(target_train_loader_aug)\n",
        "\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target_aug, _ = next(target_iter_aug)\n",
        "\n",
        "    # input_weak = torch.cat([inputs_source, inputs_target], dim=0) \n",
        "    # input_strong = torch.cat([inputs_source_aug, inputs_target_aug], dim=0)\n",
        "\n",
        "    targets_domain = torch.cat(\n",
        "        (torch.zeros(inputs_source.shape[0]), torch.ones(inputs_target.shape[0]))\n",
        "    ).unsqueeze(dim=1)\n",
        "\n",
        "    # load data into device\n",
        "    inputs_source, inputs_target = inputs_source.to(device), inputs_target.to(device)\n",
        "    # input_weak, input_strong = input_weak.to(device), input_strong.to(device)\n",
        "    inputs_target_aug = inputs_target_aug.to(device)\n",
        "    targets_source, targets_domain = targets_source.to(device), targets_domain.to(device)\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Supervised loss\n",
        "    # -----------------------------------\n",
        "\n",
        "    output_source, _, ca_source, _ = model(inputs_source)\n",
        "    loss_source = F.cross_entropy(output_source, targets_source)\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Generate artificial label\n",
        "    # -----------------------------------\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output_weak, _, _, _ = model(inputs_target)\n",
        "      max_prob, label_weak = F.softmax(output_weak, dim=1).max(dim=1)\n",
        "      mask_pseudo = (max_prob >= conf_threshold).float()\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Unsupervised pseudo loss\n",
        "    # -----------------------------------\n",
        "    \n",
        "    loss_pseudo = torch.tensor(0.).to(device)\n",
        "\n",
        "    if pseudo:\n",
        "      output_strong, feat_strong, ca_strong, _ = model(inputs_target_aug)\n",
        "      loss_pseudo = F.cross_entropy(output_strong, label_weak, reduction='none')\n",
        "      loss_pseudo = (loss_pseudo * mask_pseudo).mean()\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Domain attention consistency loss\n",
        "    # -----------------------------------\n",
        "\n",
        "    loss_attn = torch.tensor(0.).to(device)\n",
        "\n",
        "    if attn:\n",
        "      # DAC loss for the fourth residual block\n",
        "      mean_tar_ca1 = tar_ca_last1 * ema_alpha + (1. - ema_alpha) * torch.mean(ca_strong[-1], 0)\n",
        "      tar_ca_last1 = mean_tar_ca1.detach()\n",
        "\n",
        "      mean_src_ca1 = src_ca_last1 * ema_alpha + (1. - ema_alpha) * torch.mean(ca_source[-1], 0)\n",
        "      src_ca_last1 = mean_src_ca1.detach()\n",
        "\n",
        "      loss_attn += torch.mean(torch.abs(mean_src_ca1 - mean_tar_ca1))\n",
        "\n",
        "      # DAC loss for the third residual block\n",
        "      mean_tar_ca2 = tar_ca_last2 * ema_alpha + (1. - ema_alpha) * torch.mean(ca_strong[-2], 0)\n",
        "      tar_ca_last2 = mean_tar_ca2.detach()\n",
        "\n",
        "      mean_src_ca2 = src_ca_last2 * ema_alpha + (1. - ema_alpha) * torch.mean(ca_source[-2], 0)\n",
        "      src_ca_last2 = mean_src_ca2.detach()\n",
        "\n",
        "      loss_attn += torch.mean(torch.abs(mean_src_ca2 - mean_tar_ca2))\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Class compactness loss\n",
        "    # -----------------------------------\n",
        "    \n",
        "    loss_comp = torch.tensor(0.).to(device)\n",
        "\n",
        "    if comp:\n",
        "      with torch.no_grad():\n",
        "        img_shape = inputs_target.shape\n",
        "\n",
        "        # enforce class compactness loss only on target samples that have consistent prediction under Guassian noise\n",
        "        noise = torch.randn(img_shape[0], img_shape[1], img_shape[2], img_shape[3]) * 0.15\n",
        "        input_noise = torch.flip(inputs_target, [3]) + noise.to(device)\n",
        "        output_noise, feat_noise, _, _ = model(input_noise)\n",
        "        max_prob, label_noise = F.softmax(output_noise, dim=1).max(1)\n",
        "        mask_noise = (max_prob >= conf_threshold).float()\n",
        "\n",
        "        # create mask to filter samples whose prediction is not consistent under Guassian noise\n",
        "        # or that under weak augmentation or noise where predicted with confidence below threshold\n",
        "        label_cons = (label_weak == label_noise).float()\n",
        "        mask_cons = mask_pseudo * mask_noise * label_cons\n",
        "\n",
        "        if mask_cons.sum() > 0: # check if there is at least one sample that satisfy the contraints\n",
        "          weight = model.fc.weight\n",
        "          # feat_strong has shape [60, 512, 1, 1] while weight[label_weak, :] has shape [60, 512]\n",
        "          # so squeeze feat_strong to shape [60, 512], removing unnecessary dimensions\n",
        "          # moreover, mask_cons has shape [60] thus unsqueeze to [60, 1] to have same number of dimensions as other two\n",
        "          diff = mask_cons.unsqueeze(dim=1) * (feat_strong.squeeze() - weight[label_weak, :])\n",
        "          loss_comp = weight_comp * torch.mean(diff * diff)\n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Adversarial loss\n",
        "    # -----------------------------------\n",
        "\n",
        "    loss_adv = torch.tensor(0.).to(device)\n",
        "    \n",
        "    if adv:\n",
        "      ni = i + nb * epoch\n",
        "\n",
        "      r = ni / max_iterations\n",
        "      gamma = 2 / (1 + math.exp(-10 * r)) - 1\n",
        "\n",
        "      _, _, _, domain_source = model(inputs_source, gamma, gamma)\n",
        "      _, _, _, domain_target = model(inputs_target, gamma, gamma)\n",
        "\n",
        "      pred_domain = torch.cat((domain_source, domain_target))\n",
        "      true_domain = torch.cat((torch.zeros(domain_source.shape), torch.ones(domain_target.shape))).to(device)\n",
        "\n",
        "      loss_adv = F.binary_cross_entropy_with_logits(pred_domain, true_domain)    \n",
        "\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Backpropagation\n",
        "    # -----------------------------------\n",
        "\n",
        "    loss = loss_source + loss_pseudo * weight_pseudo + loss_attn * weight_attn + loss_comp * weight_comp + loss_adv * weight_adv\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print statistics\n",
        "    source_samples += inputs_source.shape[0]\n",
        "    target_samples += inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += loss_source.item()\n",
        "    _, predicted = F.softmax(output_source, dim=1).max(dim=1)\n",
        "    cumulative_accuracy += predicted.eq(targets_source).sum().item()\n",
        "\n",
        "    mloss_source = (mloss_source * i + loss_source.item()) / (i + 1)\n",
        "    mloss_pseudo = (mloss_pseudo * i + loss_pseudo.item()) / (i + 1)\n",
        "    mloss_attn = (mloss_attn * i + loss_attn.item()) / (i + 1)\n",
        "    mloss_comp = (mloss_comp * i + loss_comp.item()) / (i + 1)\n",
        "    mloss_adv = (mloss_adv * i + loss_adv.item()) / (i + 1)\n",
        "    pbar.set_description(\"Source loss {} | Pseudo loss {} | Attn loss {} | Comp loss {} | Adv loss {}\".format(round(mloss_source.item(),4), round(mloss_pseudo.item(),4), round(mloss_attn.item(),4), round(mloss_comp.item(),4), round(mloss_adv.item(),4)))\n",
        "\n",
        "    if show_img and i < 3:\n",
        "      fig = plt.figure()\n",
        "      \n",
        "      ax1 = fig.add_subplot(1,4,1)\n",
        "      ax1.imshow(np.clip(inputs_source[0].permute(1,2,0).cpu() * std + mean, 0, 1))\n",
        "      ax2 = fig.add_subplot(1,4,2)\n",
        "      ax2.imshow(np.clip(inputs_source_aug[0].permute(1,2,0).cpu() * std + mean, 0, 1))\n",
        "      ax3 = fig.add_subplot(1,4,3)\n",
        "      ax3.imshow(np.clip(inputs_target[0].permute(1,2,0).cpu() * std + mean, 0, 1))\n",
        "      ax4 = fig.add_subplot(1,4,4)\n",
        "      ax4.imshow(np.clip(inputs_target_aug[0].permute(1,2,0).cpu() * std + mean, 0, 1))\n",
        "\n",
        "      [axi.set_axis_off() for axi in [ax1, ax2, ax3, ax4]]\n",
        "      plt.show()\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_accuracy/source_samples*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1brqIylP764"
      },
      "outputs": [],
      "source": [
        "def test_step(model, target_test_loader, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(target_test_loader)):\n",
        "\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      output, _, _, _ = model(inputs)\n",
        "\n",
        "      # apply the loss\n",
        "      loss = F.cross_entropy(output, targets)\n",
        "\n",
        "      # print statistics\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = F.softmax(output, dim=1).max(dim=1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J359b2Q73hhN"
      },
      "source": [
        "# Execute everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WmZKxapc4sU"
      },
      "outputs": [],
      "source": [
        "# Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html\n",
        "# cudnn seed 0 settings are slower and more reproducible, else faster and less reproducible\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "cudnn.benchmark, cudnn.deterministic = (False, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiR2HcGtM_Am"
      },
      "outputs": [],
      "source": [
        "batch_size = 60\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 5e-4\n",
        "epochs = 20\n",
        "\n",
        "prod_root = 'adaptiope_small/product_images'\n",
        "real_root = 'adaptiope_small/real_life'\n",
        "\n",
        "dataloaders_prod, dataloaders_prod_aug, dataloaders_real, dataloaders_real_aug= get_data(batch_size, prod_root, real_root)\n",
        "\n",
        "train_loader_prod, test_loader_prod = dataloaders_prod\n",
        "train_loader_prod_aug, test_loader_prod_aug = dataloaders_prod_aug\n",
        "\n",
        "train_loader_real, test_loader_real = dataloaders_real\n",
        "train_loader_real_aug, test_loader_real_aug = dataloaders_real_aug\n",
        "\n",
        "model = get_model(pretrained=True).to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load('./model_weights.pt'))\n",
        "\n",
        "optimizer = get_optimizer(model, learning_rate)\n",
        "\n",
        "scheduler = get_scheduler(optimizer, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9P-pxqnw5T7"
      },
      "outputs": [],
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for e in range(epochs):\n",
        "  print('Epoch: {}/{}'.format(e+1, epochs))\n",
        "  train_ce_loss, train_accuracy = training_step(\n",
        "    model=model,\n",
        "    # source_train_loader=(train_loader_prod, train_loader_prod_aug),\n",
        "    # target_train_loader=(train_loader_real, train_loader_real_aug),\n",
        "    source_train_loader=(train_loader_real, train_loader_real_aug),\n",
        "    target_train_loader=(train_loader_prod, train_loader_prod_aug),\n",
        "    optimizer=optimizer,\n",
        "    pseudo=0.3, # 1.,\n",
        "    attn=0., # 0.3,\n",
        "    comp=0., # 0.1,\n",
        "    adv=1., # 0.3,\n",
        "    device=device, \n",
        "    show_img=False,\n",
        "    ep=[e, epochs]\n",
        "  )\n",
        "  \n",
        "  scheduler.step()\n",
        "  \n",
        "  test_loss, test_accuracy = test_step(\n",
        "    model=model, \n",
        "    target_test_loader=test_loader_real, \n",
        "    device=device\n",
        "  )\n",
        "  if test_accuracy > best_accuracy:\n",
        "    # torch.save(model.state_dict(), './model_weights.pt')\n",
        "    best_accuracy = test_accuracy\n",
        "\n",
        "  history['train_acc'].append(train_accuracy)\n",
        "  history['train_loss'].append(train_ce_loss)\n",
        "  history['test_acc'].append(test_accuracy)\n",
        "  history['test_loss'].append(test_loss)\n",
        "  # print('Train: CE loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_accuracy))\n",
        "  print('Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cl2k9rzrwCB"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['test_acc'], label='test accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LalwBzQlx1AY"
      },
      "source": [
        "# Accuracy notes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBgjrkvc7UY"
      },
      "source": [
        "*Running experiments with seed = 0, pretrained = True, batch size = 60, optimizer = Adam, learning_rate = 5e-4, scheduler = cosine and epochs = 20.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rs9aRjQsVud"
      },
      "source": [
        "### Prod &rarr; Real\n",
        "\n",
        "*   Baseline (source loss): 64.16 (last) 67.17 (best)\n",
        "\n",
        "---\n",
        "\n",
        "*   DA (source loss + pseudo loss): 73.68 (last) 74.44 (best)\n",
        "*   DA (source loss + pseudo loss + attn loss): 75.69 (last) 75.69 (best)\n",
        "*   DA (source loss + pseudo loss + attn loss + comp loss): 76.44 (last) 77.69 (best)\n",
        "\n",
        "---\n",
        "\n",
        "*   DA (source loss + adv loss): 66.92 (last) 66.92 (best)\n",
        "*   DA (source loss + gamma-adv loss):  (last)  (best)\n",
        "*   DA (source loss + adv-attn loss): 64.16 (last) 64.16 (best)\n",
        "*   DA (source loss + gammas-adv-attn loss): 69.17 (last) 71.93 (best)\n",
        "*   DA (source loss + pseudo loss + 0.3*gammas-adv-attn loss): 78.95 (last) 80.20 (best)\n",
        "*   DA (source loss + 0.1*pseudo loss + gammas-adv-attn loss): 73.43 (last) 75.19 (best)\n",
        "*   DA (source loss + pseudo loss + gammas-adv-attn loss): 78.45 (last) 78.95 (best)\n",
        "*   DA (source loss + 0.1*pseudo loss + adv loss):  (last)  (best)\n",
        "*   DA (source loss + 0.1*pseudo loss + adv-attn loss): 77.94 (last) 77.94 (best)\n",
        "*   DA (source loss + 0.5*pseudo loss + adv-attn loss): 81.20 (last) 81.20 (best)\n",
        "*   DA (source loss + 0.8*pseudo loss + adv-attn loss): 75.94 (last) 76.69 (best)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real &rarr; Prod\n",
        "\n",
        "*   Baseline (source loss): 87.47 (last) 87.47 (best)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   DA (source loss + pseudo loss): 88.22 (last) 89.22 (best)\n",
        "*   DA (source loss + pseudo loss + attn loss): 86.97 (last) 88.97 (best)\n",
        "*   DA (source loss + pseudo loss + attn loss + comp loss): 84.96 (last) 85.21 (best)\n",
        "\n",
        "---\n",
        "\n",
        "*   DA (source loss + adv loss): 87.97 (last) 89.72 (best)\n",
        "*   DA (source loss + gamma-adv loss):  (last)  (best)\n",
        "*   DA (source loss + adv-attn loss): 88.72 (last) 89.72 (best)\n",
        "*   DA (source loss + gammas-adv-attn loss): 86.72 (last) 87.97 (best)\n",
        "*   DA (source loss + 0.1*pseudo loss + gammas-adv-attn loss): 88.47 (last) 88.47 (best)\n",
        "*   DA (source loss + 0.3*pseudo loss + gammas-adv-attn loss): 88.97 (last) 89.22 (best)\n",
        "*   DA (source loss + pseudo loss + 0.3*gammas-adv-attn loss): 86.22 (last) 89.22 (best)\n",
        "*   DA (source loss + pseudo loss + gammas-adv-attn loss): 86.22 (last) 87.47 (best)\n",
        "*   DA (source loss + 0.1*pseudo loss + adv-attn loss): 89.22 (last) 90.23 (best)\n",
        "*   DA (source loss + 0.5*pseudo loss + adv-attn loss): 88.22 (last) 88.97 (best)\n",
        "*   DA (source loss + 0.8*pseudo loss + adv-attn loss): 86.72 (last) 87.97 (best)"
      ],
      "metadata": {
        "id": "mFOlhz21wVKl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwrEv5W3wTbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}