{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/DL_Assignment/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ffPEwcd91gLA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NH17GuP4Kp0",
        "outputId": "e2dcf514-39a9-41eb-9033-4503e1f5e229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzobgOes1r5q"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Adaptiope.zip\" dataset/\n",
        "!ls dataset\n",
        "!unzip dataset/Adaptiope.zip\n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MXYoK2ks8l0A"
      },
      "outputs": [],
      "source": [
        "!mkdir adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dZm5jySz1wRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da5a5bc-6af1-437f-d1a9-9bf4e0f67b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  8.02it/s]\n",
            "100%|██████████| 20/20 [00:04<00:00,  4.56it/s]\n"
          ]
        }
      ],
      "source": [
        "classes = os.listdir(\"Adaptiope/product_images\")\n",
        "classes = [\"/backpack\", \"/bookcase\", \"/car jack\", \"/comb\", \"/crown\", \"/file cabinet\", \"/flat iron\", \"/game controller\", \"/glasses\",\n",
        "           \"/helicopter\", \"/ice skates\", \"/letter tray\", \"/monitor\", \"/mug\", \"/network switch\", \"/over-ear headphones\", \"/pen\",\n",
        "           \"/purse\", \"/stand mixer\", \"/stroller\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  os.makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = ''.join((d, c))\n",
        "    c_target = ''.join((td, c))\n",
        "    shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PD5EnkoSCZN7"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, product_root, real_root):\n",
        "  # resizing and cropping\n",
        "  # prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                 # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # normalize with ImageNet mean\n",
        "  transform = T.Compose(transform)                            # compose the above transformations into one\n",
        "    \n",
        "  # load data\n",
        "  dataset_prod = torchvision.datasets.ImageFolder(root=product_root, transform=transform)\n",
        "  dataset_real = torchvision.datasets.ImageFolder(root=real_root, transform=transform)\n",
        "  \n",
        "  # create train and test splits (80/20)\n",
        "  num_samples = len(dataset_prod) # same number of samples in this dataset\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  train_data_prod, test_data_prod = torch.utils.data.random_split(dataset_prod, [training_samples, test_samples])\n",
        "  train_data_real, test_data_real = torch.utils.data.random_split(dataset_real, [training_samples, test_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader_prod = torch.utils.data.DataLoader(train_data_prod, batch_size, shuffle=True)\n",
        "  test_loader_prod = torch.utils.data.DataLoader(test_data_prod, batch_size, shuffle=False)\n",
        "  \n",
        "  train_loader_real = torch.utils.data.DataLoader(train_data_real, batch_size, shuffle=True)\n",
        "  test_loader_real = torch.utils.data.DataLoader(test_data_real, batch_size, shuffle=False)\n",
        "  \n",
        "  return (train_loader_prod, test_loader_prod), (train_loader_real, test_loader_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Create model"
      ],
      "metadata": {
        "id": "snIOIHPPw6K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DLSA(torch.nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.backbone = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "    self.linear1 = nn.Linear(1000, 512)\n",
        "    self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.linear2 = nn.Linear(512, 512)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.dropout = nn.Dropout()\n",
        "    self.linear3 = nn.Linear(512, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.batch_norm1(x)\n",
        "\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.batch_norm2(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    g_x = self.linear3(x)\n",
        "\n",
        "    y_x = F.softmax(g_x)\n",
        "    \n",
        "    return g_x, y_x"
      ],
      "metadata": {
        "id": "N0FnIZglw48I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "28LBcVrtKiGd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "\n",
        "# def get_entropy_loss(x):\n",
        "#   p = F.softmax(x, dim=1)\n",
        "#   q = F.log_softmax(x, dim=1)\n",
        "#   b = p * q\n",
        "#   b = -1.0 * b.sum(-1).mean()\n",
        "#   return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "454SYDrAKqTE"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  # iterate through the layers of the network\n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('fc'):\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "  \n",
        "  # assign the distinct learning rates to each group of parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr=lr/10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_a_z_b_z(g_z):\n",
        "  L_v = g_z[..., 0:1] # 1st element\n",
        "  L_w = g_z[..., 1:] # all elements except 1st\n",
        "  L_v_mean = torch.mean(L_v, axis=0, keepdim=True) # mean over samples\n",
        "  L_w_mean = torch.mean(L_w, axis=0, keepdim=True) # mean over samples\n",
        "\n",
        "  nominator = torch.sum(L_v*L_w - L_v_mean*L_w_mean, axis=0, keepdim=True) / g_z.shape[0]\n",
        "  denominator = torch.sum(L_v - L_v_mean**2, axis=0, keepdim=True) / g_z.shape[0]\n",
        "\n",
        "  a_z = nominator / denominator\n",
        "  b_z = L_w_mean - a_z*L_v_mean\n",
        "\n",
        "  return a_z, b_z"
      ],
      "metadata": {
        "id": "mibcHrnpywsi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_adaptation_loss(a_s, a_t, b_s, b_t, gamma):\n",
        "  inner_product = (a_s * a_t).sum(dim=1)\n",
        "  a_s_norm = torch.norm(a_s)\n",
        "  a_t_norm = torch.norm(a_t)\n",
        "  cos = inner_product / (a_s_norm * a_t_norm)\n",
        "  angle = torch.acos(cos)\n",
        "\n",
        "  return torch.deg2rad(angle) + gamma*torch.norm(b_s - b_t)"
      ],
      "metadata": {
        "id": "zpuFAmMzywxd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elements_for_each_class(g, y, elements):\n",
        "  for i, val in enumerate(y):\n",
        "    # get index of the max --> most likely class\n",
        "    index = torch.argmax(val)\n",
        "    elements[index].append(g[i])\n",
        "  return elements"
      ],
      "metadata": {
        "id": "KSLdY_4PIsga"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_conditional_adaptation_loss(g_s, y_s, g_t, y_t, gamma, num_classes):\n",
        "  # create a fixed size array. For each element of array create a new list\n",
        "\n",
        "  elements_for_class_s = [list()] * num_classes\n",
        "  elements_for_class_t = [list()] * num_classes\n",
        "\n",
        "  elements_for_class_s = get_elements_for_each_class(g_s, y_s, elements_for_class_s)\n",
        "  elements_for_class_t = get_elements_for_each_class(g_t, y_t, elements_for_class_t)\n",
        "\n",
        "  sum_a = sum_b = 0\n",
        "  for num_class in range(num_classes):\n",
        "    a_s,b_s = compute_a_z_b_z(elements_for_class_s[num_classes])\n",
        "    a_t,b_t = compute_a_z_b_z(elements_for_class_t[num_classes])\n",
        "    sum_a += np.linalg.norm(a_s - a_t)**2\n",
        "    sum_b += np.linalg.norm(b_s - b_t)**2\n",
        "  \n",
        "  return sum_a/num_classes + gamma* (sum_b/num_classes)\n",
        "  "
      ],
      "metadata": {
        "id": "WHmveltH7y3A"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XvOkDNmHLDz9"
      },
      "outputs": [],
      "source": [
        "def training_step(model, source_train_loader, target_train_loader, optimizer, \n",
        "                  cost_function, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  gamma = 0.1\n",
        "  alpha = 0.2\n",
        "  \n",
        "  target_iter = iter(target_train_loader)\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.train()\n",
        "  pbar = tqdm(source_train_loader)\n",
        "\n",
        "  mloss_ce = torch.zeros(1)\n",
        "  mloss_lm = torch.zeros(1)\n",
        "  for i, (inputs_source, targets) in enumerate(pbar):\n",
        "    \n",
        "    # get target data. If the target iterator reaches the end, restart it\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_train_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    \n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    g_x, y_x = model(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    g_s, g_t = torch.split(g_x, split_size_or_sections=inputs_source.shape[0], dim=0)\n",
        "    y_s, y_t = torch.split(y_x, split_size_or_sections=inputs_source.shape[0], dim=0)\n",
        "    \n",
        "    a_s, b_s = compute_a_z_b_z(g_s)\n",
        "    a_t, b_t = compute_a_z_b_z(g_t)\n",
        "\n",
        "    # apply the losses\n",
        "    ce_loss = cost_function(y_s, targets)\n",
        "    lm_loss = compute_adaptation_loss(a_s, a_t, b_s, b_t, gamma)\n",
        "    conditional_adaptation_loss = compute_conditional_adaptation_loss(g_s, y_s, g_t, y_t, gamma, 20)\n",
        "\n",
        "    if torch.isnan(lm_loss):\n",
        "      return a_s, a_t, b_s, b_t\n",
        "    \n",
        "    # need to add last loss\n",
        "    loss = ce_loss + (1-alpha)*lm_loss + conditional_adaptation_loss\n",
        "    \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print statistics\n",
        "    source_samples += inputs_source.shape[0]\n",
        "    target_samples += inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    _, predicted = y_s.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    mloss_ce = (mloss_ce * i + ce_loss.item()) / (i + 1)\n",
        "    mloss_lm = (mloss_lm * i + lm_loss.item()) / (i + 1)\n",
        "\n",
        "    pbar.set_description(\"CE loss {} | LM loss {}\".format(round(mloss_ce.item(),4), round(mloss_lm.item(),4)))\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_accuracy/source_samples*100\n",
        "\n",
        "\n",
        "def test_step(model, target_test_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(target_test_loader)):\n",
        "\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      g_x, y_x = model(inputs)\n",
        "\n",
        "      # apply the loss\n",
        "      loss = cost_function(y_x, targets)\n",
        "\n",
        "      # print statistics\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = y_x.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ihVImByqLZds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "eb4d03be-7461-44b6-eee1-4d57cc4407d1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-cb7a34ea8265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_prod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDLSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.34 GiB already allocated; 3.75 MiB free; 13.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "batch_size = 20\n",
        "device='cuda:0'\n",
        "learning_rate=0.01\n",
        "weight_decay=0.000001\n",
        "momentum=0.9\n",
        "epochs=10\n",
        "\n",
        "prod_root = 'adaptiope_small/product_images'\n",
        "real_root = 'adaptiope_small/real_life'\n",
        "\n",
        "dataloaders_prod, dataloaders_real = get_data(batch_size, prod_root, real_root)\n",
        "train_loader_prod, test_loader_prod = dataloaders_real\n",
        "train_loader_real, test_loader_real = dataloaders_prod\n",
        "\n",
        "num_classes = len(set(train_loader_prod.dataset.dataset.targets))\n",
        "\n",
        "model = DLSA(num_classes).to(device)\n",
        "\n",
        "optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n",
        "\n",
        "cost_function = get_cost_function()\n",
        "\n",
        "for e in range(epochs):\n",
        "  print('Epoch: {}/{}'.format(e+1, epochs))\n",
        "  a_s, a_t, b_s, b_t = training_step(model=model,\n",
        "                                                source_train_loader=train_loader_prod,\n",
        "                                                target_train_loader=train_loader_real,\n",
        "                                                optimizer=optimizer, \n",
        "                                                cost_function=cost_function,\n",
        "                                                device=device)\n",
        "  \n",
        "  test_loss, test_accuracy = test_step(model=model, \n",
        "                                       target_test_loader=test_loader_real, \n",
        "                                       cost_function=cost_function, \n",
        "                                       device=device)\n",
        "  \n",
        "  print('Train: CE loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_accuracy))\n",
        "  print('Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}