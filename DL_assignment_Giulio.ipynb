{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/DL_Assignment/blob/main/DL_assignment_Giulio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY03c6gbuYAT"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nCxw3XdPwwv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Y8geD1uccJ"
      },
      "source": [
        "# Extract data and create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C4tGtJuITp1"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn9EIurhutPN"
      },
      "outputs": [],
      "source": [
        "!unzip -q -o gdrive/MyDrive/Adaptiope.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPGsSMGRdHX"
      },
      "outputs": [],
      "source": [
        "classes = [\"/backpack\", \"/bookcase\", \"/car jack\", \"/comb\", \"/crown\", \"/file cabinet\", \"/flat iron\", \"/game controller\", \"/glasses\",\n",
        "           \"/helicopter\", \"/ice skates\", \"/letter tray\", \"/monitor\", \"/mug\", \"/network switch\", \"/over-ear headphones\", \"/pen\",\n",
        "           \"/purse\", \"/stand mixer\", \"/stroller\"]\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  os.makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = ''.join((d, c))\n",
        "    c_target = ''.join((td, c))\n",
        "    shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrMqpSCyxwl"
      },
      "source": [
        "# Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fkEbfTGVE-0"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, product_root, real_root):\n",
        "  # resizing and cropping\n",
        "  # prepare data transformations for the train loader\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                 # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                               std=[0.229, 0.224, 0.225]))    # normalize with ImageNet mean\n",
        "  transform = T.Compose(transform)                            # compose the above transformations into one\n",
        "    \n",
        "  # load data\n",
        "  dataset_prod = torchvision.datasets.ImageFolder(root=product_root, transform=transform)\n",
        "  dataset_real = torchvision.datasets.ImageFolder(root=real_root, transform=transform)\n",
        "  \n",
        "  # create train and test splits (80/20)\n",
        "  num_samples = len(dataset_prod) # same number of samples in this dataset\n",
        "  training_samples = int(num_samples * 0.8 + 1)\n",
        "  test_samples = num_samples - training_samples\n",
        "\n",
        "  train_data_prod, test_data_prod = torch.utils.data.random_split(dataset_prod, [training_samples, test_samples])\n",
        "  train_data_real, test_data_real = torch.utils.data.random_split(dataset_real, [training_samples, test_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader_prod = torch.utils.data.DataLoader(train_data_prod, batch_size, shuffle=True)\n",
        "  test_loader_prod = torch.utils.data.DataLoader(test_data_prod, batch_size, shuffle=False)\n",
        "  \n",
        "  train_loader_real = torch.utils.data.DataLoader(train_data_real, batch_size, shuffle=True)\n",
        "  test_loader_real = torch.utils.data.DataLoader(test_data_real, batch_size, shuffle=False)\n",
        "  \n",
        "  return (train_loader_prod, test_loader_prod), (train_loader_real, test_loader_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSH5iS7K1C2I"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIlGuzqLNigi"
      },
      "outputs": [],
      "source": [
        "class DLSA(torch.nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.backbone = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "    self.linear1 = nn.Linear(1000, 512)\n",
        "    self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.linear2 = nn.Linear(512, 512)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.dropout = nn.Dropout()\n",
        "    self.linear3 = nn.Linear(512, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    \n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.batch_norm1(x)\n",
        "\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.batch_norm2(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    g_x = self.linear3(x)\n",
        "\n",
        "    y_x = F.softmax(g_x)\n",
        "    \n",
        "    return g_x, y_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpK-Bq6d1FnV"
      },
      "source": [
        "# Specify cost function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLj-c_tPIgCM"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMpc8dxrMmWc"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "  # we will create two groups of weights, one for the newly initialized layer\n",
        "  # and the other for rest of the layers of the network\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  # iterate through the layers of the network\n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('fc'):\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "  \n",
        "  # assign the distinct learning rates to each group of parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr=lr/10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_I6N6U81MDA"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SuLjlmcXTz6"
      },
      "outputs": [],
      "source": [
        "def compute_a_z_b_z(g_z):\n",
        "  L_v = g_z[..., 0:1] # 1st element of all samples\n",
        "  L_w = g_z[..., 1:] # all elements, except 1st, of all samples\n",
        "  L_v_mean = torch.mean(L_v, axis=0, keepdim=True) # mean over samples\n",
        "  L_w_mean = torch.mean(L_w, axis=0, keepdim=True) # mean over samples\n",
        "\n",
        "  nominator = torch.sum(L_v*L_w - L_v_mean*L_w_mean, axis=0, keepdim=True) / g_z.shape[0]\n",
        "  denominator = torch.sum(L_v - L_v_mean**2, axis=0, keepdim=True) / g_z.shape[0]\n",
        "\n",
        "  a_z = nominator / denominator\n",
        "  b_z = L_w_mean - a_z*L_v_mean\n",
        "\n",
        "  return a_z, b_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlisysWrrr8-"
      },
      "outputs": [],
      "source": [
        "def compute_adaptation_loss(a_s, a_t, b_s, b_t, gamma):\n",
        "  inner_product = (a_s * a_t).sum(dim=1)\n",
        "  a_s_norm = torch.norm(a_s)\n",
        "  a_t_norm = torch.norm(a_t)\n",
        "  cos = inner_product / (a_s_norm * a_t_norm + 1e-9) # add small number to avoid dividing by 0\n",
        "  angle = torch.acos(cos)\n",
        "\n",
        "  return torch.deg2rad(angle) + gamma*torch.norm(b_s - b_t)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCThdd1kHqQR"
      },
      "outputs": [],
      "source": [
        "def compute_conditional_loss(g_s, g_t, classes_s, classes_t, gamma):\n",
        "  lc_loss = torch.zeros(1, device=device)\n",
        "  num_classes = 0\n",
        "\n",
        "  for c in torch.cat((classes_s, classes_t)).unique():\n",
        "    g_s_c = g_s[(classes_s == c).nonzero(as_tuple=True)]\n",
        "    g_t_c = g_t[(classes_t == c).nonzero(as_tuple=True)]\n",
        "\n",
        "    # if one of the domains has less than one sample with class c, skip this class\n",
        "    if not len(g_s_c) or not len(g_t_c):\n",
        "      continue\n",
        "\n",
        "    a_s_c, b_s_c = compute_a_z_b_z(g_s_c)\n",
        "    a_t_c, b_t_c = compute_a_z_b_z(g_t_c)\n",
        "\n",
        "    lc_loss += compute_adaptation_loss(a_s_c, a_t_c, b_s_c, b_t_c, gamma)\n",
        "    num_classes += 1\n",
        "\n",
        "  return lc_loss/num_classes if num_classes else lc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrHToapkZcKD"
      },
      "outputs": [],
      "source": [
        "def training_step(model, source_train_loader, target_train_loader, optimizer, \n",
        "                  cost_function, device='cuda:0'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  gamma = 0.1\n",
        "  alpha = 0.2\n",
        "  \n",
        "  target_iter = iter(target_train_loader)\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.train()\n",
        "  pbar = tqdm(source_train_loader)\n",
        "\n",
        "  mloss_ce = torch.zeros(1)\n",
        "  mloss_lm = torch.zeros(1)\n",
        "  mloss_lc = torch.zeros(1)\n",
        "  for i, (inputs_source, targets) in enumerate(pbar):\n",
        "    \n",
        "    # get target data. If the target iterator reaches the end, restart it\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_train_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "    \n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    g_x, y_x = model(inputs)\n",
        "    \n",
        "    # split the source and target outputs\n",
        "    g_s, g_t = torch.split(g_x, split_size_or_sections=inputs_source.shape[0], dim=0)\n",
        "    y_s, y_t = torch.split(y_x, split_size_or_sections=inputs_source.shape[0], dim=0)\n",
        "    \n",
        "    a_s, b_s = compute_a_z_b_z(g_s)\n",
        "    a_t, b_t = compute_a_z_b_z(g_t)\n",
        "\n",
        "    # apply the losses\n",
        "    ce_loss = cost_function(y_s, targets)\n",
        "    lm_loss = compute_adaptation_loss(a_s, a_t, b_s, b_t, gamma)\n",
        "    lc_loss = compute_conditional_loss(g_s, g_t, targets, torch.argmax(y_t, dim=1), gamma)\n",
        "    \n",
        "    loss = ce_loss + 0.01*lm_loss + 0.001*lc_loss\n",
        "    \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # print statistics\n",
        "    source_samples += inputs_source.shape[0]\n",
        "    target_samples += inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_ce_loss += ce_loss.item()\n",
        "    _, predicted = y_s.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    mloss_ce = (mloss_ce * i + ce_loss.item()) / (i + 1)\n",
        "    mloss_lm = (mloss_lm * i + lm_loss.item()) / (i + 1)\n",
        "    mloss_lc = (mloss_lc * i + lc_loss.item()) / (i + 1)\n",
        "\n",
        "    pbar.set_description(\"CE loss {} | LM loss {} | LC loss {}\".format(round(mloss_ce.item(),4), round(mloss_lm.item(),4), round(mloss_lc.item(),4)))\n",
        "\n",
        "  return cumulative_ce_loss/source_samples, cumulative_accuracy/source_samples*100\n",
        "\n",
        "\n",
        "def test_step(model, target_test_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # strictly needed if network contains layers which has different behaviours between train and test\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(target_test_loader)):\n",
        "\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      g_x, y_x = model(inputs)\n",
        "\n",
        "      # apply the loss\n",
        "      loss = cost_function(y_x, targets)\n",
        "\n",
        "      # print statistics\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = y_x.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J359b2Q73hhN"
      },
      "source": [
        "# Execute everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WmZKxapc4sU"
      },
      "outputs": [],
      "source": [
        "# Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html\n",
        "# cudnn seed 0 settings are slower and more reproducible, else faster and less reproducible\n",
        "seed = 0\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "cudnn.benchmark, cudnn.deterministic = (False, True) if seed == 0 else (True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiR2HcGtM_Am"
      },
      "outputs": [],
      "source": [
        "batch_size = 60\n",
        "device='cuda:0'\n",
        "learning_rate=0.01\n",
        "weight_decay=0.000001\n",
        "momentum=0.9\n",
        "epochs=10\n",
        "\n",
        "prod_root = 'adaptiope_small/product_images'\n",
        "real_root = 'adaptiope_small/real_life'\n",
        "\n",
        "dataloaders_prod, dataloaders_real = get_data(batch_size, prod_root, real_root)\n",
        "train_loader_prod, test_loader_prod = dataloaders_real\n",
        "train_loader_real, test_loader_real = dataloaders_prod\n",
        "\n",
        "num_classes = len(set(train_loader_prod.dataset.dataset.targets))\n",
        "\n",
        "model = DLSA(num_classes).to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load('./model2losses.pt'))\n",
        "\n",
        "optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n",
        "\n",
        "cost_function = get_cost_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u9P-pxqnw5T7"
      },
      "outputs": [],
      "source": [
        "for e in range(epochs):\n",
        "  print('Epoch: {}/{}'.format(e+1, epochs))\n",
        "  train_ce_loss, train_accuracy = training_step(model=model,\n",
        "                                                source_train_loader=train_loader_prod,\n",
        "                                                target_train_loader=train_loader_real,\n",
        "                                                optimizer=optimizer, \n",
        "                                                cost_function=cost_function,\n",
        "                                                device=device)\n",
        "  \n",
        "  test_loss, test_accuracy = test_step(model=model, \n",
        "                                       target_test_loader=test_loader_real, \n",
        "                                       cost_function=cost_function, \n",
        "                                       device=device)\n",
        "  \n",
        "  # print('Train: CE loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_accuracy))\n",
        "  print('Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFYmoawTReyx"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), './.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LalwBzQlx1AY"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvyw-GKXLVo0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_assignment_Giulio.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}